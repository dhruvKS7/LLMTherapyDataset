As large language models (LLMs) become increasingly powerful and integrated into daily life, their ability to engage in conversations involving human emotion and meaningful discussions about mental health requires closer examination. Prior research has focused on where LLMs succeed and fail in this task, but there is little literature specifically dedicated to understanding how to make them more effective. To bridge this gap, we conducted extensive research into the current landscape of LLMs acting as therapists in an effort to understand what factors contribute to making them as useful as possible. Our research led us to the core finding that fine-tuning models can increase their effectiveness to a large degree. Interestingly, however, there was little information explaining how to actually fine-tune models for something as intricate, private, and personal as mental health. As such, our contribution is a comprehensive, extensible, and accessible pipeline for fine-tuning LLMs on publicly available client-therapist transcripts. The efficacy of our pipeline was validated through extensive evaluation centered around comparing how well our fine-tuned model replicated human therapist responses compared to a base LLM. Through our pipeline and evaluation, we detail a way to make LLMs better at acting as therapists and a way to automatically test their effectiveness without requiring large-scale user studies.
